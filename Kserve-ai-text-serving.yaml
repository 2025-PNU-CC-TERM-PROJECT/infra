apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: ai-text-serving
  namespace: ms-models
spec:
  predictor:
    containers:
      - name: kserve-container
        image: khuni1/ai-text-serving:latest
        ports:
          - containerPort: 8080
        resources:
          requests:
            cpu: "100m"
            memory: "1.5Gi"
            ephemeral-storage: "500Mi"
          limits:
            cpu: "250m"
            memory: "2Gi"
            ephemeral-storage: "1Gi"
